{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LUCAS Processing\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "        This notebook refers to the studies presented in [3] and to <b>Chapter 4</b> of the Ph.D. thesis [4].\n",
    "    With this notebook, you can process the raw LUCAS 2012 soil dataset to work with the 1D CNNs [2].\n",
    "    We can not guarantee completeness or correctness of the code.\n",
    "    If you find bugs or if you have suggestions on how to improve the code, we encourage you to post your ideas as <a href=\"https://github.com/felixriese/lucas-processing/issues\">GitHub issue</a>.\n",
    "</div>\n",
    "    \n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lucas_processing as lucas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE path to data\n",
    "path_to_data = r\"../data/\"\n",
    "\n",
    "# CHANGE verbosity\n",
    "verbose = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE if you don't know what you are doing\n",
    "path_to_lucas_csv = path_to_data+\"0_LUCAS_TOPSOIL_v1_spectral.csv\"\n",
    "path_to_datafile = path_to_data+\"0_LUCAS_TOPSOIL_v1.csv\"\n",
    "path_to_chunk_concat = path_to_data+\"1_lucas_fromchunks.csv\"\n",
    "path_to_combined = path_to_data+\"2_lucas_fromchunks_combined.csv\"\n",
    "path_to_full = path_to_data+\"3_lucas_full.csv\"\n",
    "path_to_final = path_to_data+\"4_lucas_final.csv\"\n",
    "path_to_subsets = path_to_data+\"5_lucas_final_\"\n",
    "path_to_chunks = path_to_data+\"chunks/\"\n",
    "path_to_data = path_to_data+\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n",
    "All of the following steps are needed to process the LUCAS dataset.\n",
    "\n",
    "### 1. Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucas.dimension_reduction(\n",
    "    csv_path=path_to_lucas_csv,\n",
    "    path_to_chunks=path_to_chunks,\n",
    "    chunksize=1000,\n",
    "    verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Divide dataset into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucas.concat_chunks(\n",
    "    path_to_chunks=path_to_chunks,\n",
    "    output_file=path_to_chunk_concat,\n",
    "    verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Combine hyperspectral samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucas.combine_hyp_samples(\n",
    "    hyp_csv_path=path_to_chunk_concat,\n",
    "    output_path=path_to_combined,\n",
    "    verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Combine CSV and XLS files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucas.combine_csv_xls(\n",
    "    hyp_csv_path=path_to_combined,\n",
    "    other_csv_path=path_to_datafile,\n",
    "    output_path=path_to_full,\n",
    "    verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Add categories and superclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucas.add_categories_and_superclasses(\n",
    "    input_path=path_to_full,\n",
    "    output_path=path_to_final,\n",
    "    verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucas.split_lucas_dataset(\n",
    "    full_dataset_path=path_to_final,\n",
    "    output_path_prefix=path_to_subsets,\n",
    "    random_state=42,\n",
    "    train_frac=0.8,\n",
    "    val_frac=0.2,\n",
    "    verbose=verbose)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
